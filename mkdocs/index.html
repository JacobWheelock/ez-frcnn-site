<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="description" content="None" /><link rel="canonical" href="https://ez-frcnn.github.io/" />
      <link rel="shortcut icon" href="img/favicon.ico" />
    <title>EZ-FRCNN</title>
    <link rel="stylesheet" href="css/theme.css" />
    <link rel="stylesheet" href="css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="assets/_mkdocstrings.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Getting Started";
        var mkdocs_page_input_path = "index.md";
        var mkdocs_page_url = "/";
      </script>
    
    <!--[if lt IE 9]>
      <script src="js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="." class="icon icon-home"> EZ-FRCNN
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="./search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="#">Getting Started</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#features">Features</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#requirements">Requirements</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#installation-windows">Installation (Windows)</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#installation-macos">Installation (MacOS)</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#getting-started">Getting Started</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#step-1-annotation-labeling-your-images">Step 1: Annotation - Labeling your Images</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#step-2-training-teaching-the-model-to-recognize-your-objects">Step 2: Training – Teaching the Model to Recognize Your Objects</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#step-3-inference-using-the-model-to-detect-objects-in-new-images">Step 3: Inference – Using the Model to Detect Objects in New Images</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#additional-tips-for-success">Additional Tips for Success</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#references">References</a>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="annotation/">ez-frcnn.annotation</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="characterize/">ez-frcnn.characterize</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="image_augs/">ez-frcnn.image_augs</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="inferencing/">ez-frcnn.inferencing</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="plotting/">ez-frcnn.plotting</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="training/">ez-frcnn.training</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="utils/">ez-frcnn.utils</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href=".">EZ-FRCNN</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Getting Started</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="documentation-for-ez-frcnn-a-fast-accessible-and-robust-deep-learning-package-for-object-detection-applications-in-ethology-and-cell-biology">Documentation for EZ-FRCNN: A Fast, Accessible and Robust Deep Learning Package for Object Detection Applications in Ethology and Cell Biology</h1>
<p>EZ-FRCNN is a user-friendly implementation of the popular Faster Region-based Convolutional Neural Network (Faster R-CNN) originally developed by <a href="https://ieeexplore.ieee.org/document/7485869">Ren et al</a>. This algorithm is designed <em>by</em> biologists <em>for</em> biologists, and is applicable to virtually any dataset! To get started, visit <a href="www.ezfrcnn.com">www.ezfrcnn.com</a> or continue reading below.</p>
<h2 id="features">Features</h2>
<ul>
<li>Easy installation and environment setup for all OSes</li>
<li>Jupyter Notebook &amp; full GUI available</li>
<li>Simple in-house annotation tools</li>
<li><strong>Fast training</strong>: around 1 hour on a single GPU for a standard dataset</li>
<li><strong>Fast inferencing</strong>: around 15 FPS on a single GPU</li>
<li>Significant documentation and tutorials for use</li>
</ul>
<h2 id="requirements">Requirements</h2>
<p>We provide instructions for installing EZ-FRCNN on Windows or MacOS below. While a GPU is <strong>highly recommended</strong> to use EZ-FRCNN, it is not required.</p>
<h2 id="installation-windows">Installation (Windows)</h2>
<ol>
<li>Install <a href="https://docs.docker.com/desktop/setup/install/windows-install/">Docker for Windows</a>.</li>
<li>Launch Docker Desktop.</li>
<li><a href="https://minhaskamal.github.io/DownGit/#/home?url=https://github.com/JacobWheelock/ez-frcnn">Download</a> or clone this repository.</li>
<li>Extract the contents of the ZIP file downloaded in the last step to a folder of your choice (SKIP if you used <code>git clone</code>).</li>
<li>Open the EZ-FRCNN folder and double-click <code>ez-frcnn.bat</code> to launch EZ-FRCNN. OR for a <strong>more user-friendly experience</strong>, double-click <code>ez-frcnnPane.bat</code> to launch the GUI.</li>
</ol>
<h2 id="installation-macos">Installation (MacOS)</h2>
<ol>
<li>Navigate to our <a href="https://github.com/JacobWheelock/ez-frcnn/tree/mac">Mac branch of this repository</a>.</li>
<li>Install <a href="https://docs.docker.com/desktop/install/mac-install/">Docker for MacOS</a>.</li>
<li>Launch Docker Desktop.</li>
<li><a href="https://minhaskamal.github.io/DownGit/#/home?url=https://github.com/JacobWheelock/ez-frcnn/tree/mac">Download</a> or clone the <a href="https://github.com/JacobWheelock/ez-frcnn/tree/mac">Mac branch of this repository</a>.</li>
<li>Extract the contents of the ZIP file downloaded in the last step to a folder of your choice (SKIP if you used <code>git clone</code>).</li>
<li>Open the EZ-FRCNN folder, double click <code>ez-frcnn.dmg</code>, and drag the <code>.app</code> file into your current working folder.</li>
<li>Double-click <code>ez-frcnn.app</code> to launch EZ-FRCNN. OR for a <strong>more user-friendly experience</strong>, double-click <code>ez-frcnnPane.dmg</code> to launch the GUI.</li>
</ol>
<h2 id="getting-started">Getting Started</h2>
<h3 id="step-1-annotation-labeling-your-images">Step 1: Annotation - Labeling your Images</h3>
<p>Annotation is the process of labeling the objects in your images that you want the model to recognize. This might include specific structures, organisms, or other items you’re interested in identifying.
1. <strong>Select Images to Annotate</strong>: Before opening the app, place all images you would like to annotate to the <code>ez-frcnn/annotations</code> folder.
2. <strong>Select Your Classes</strong>: After opening the annotation app, choose the classes (category) you want to annotate. For example, if you’re labeling cells, you can create classes like “nucleus” or “cell membrane.”
3. <strong>Draw Bounding Boxes</strong>: With your class selected, draw a box around each object in the image that belongs to that class. Repeat this step for each class you want the model to learn.
4. <strong>Save Your Annotations</strong>: Once all objects in an image are labeled, save your annotations. You’ll repeat these steps for a few images to give the model enough examples to learn from.</p>
<blockquote>
<p>Tip: Annotation can take a bit of time, but the more images you label, the better your model will perform. Aim for at least 20-50 labeled images to get started.</p>
</blockquote>
<h3 id="step-2-training-teaching-the-model-to-recognize-your-objects">Step 2: Training – Teaching the Model to Recognize Your Objects</h3>
<p>Once your images are annotated, you’re ready to train the model. Training is where the model learns to recognize your labeled objects based on the examples you provided.
1. <strong>Training and Validation Sets</strong>: During training, your data is split into two sets:
    - <strong>Training Set</strong>: This set is used to teach the model how to recognize your objects.
    - <strong>Validation Set</strong>: This set checks the model’s learning progress on new images it hasn’t seen, helping to ensure it’s generalizing rather than memorizing.</p>
<pre><code>Both sets are crucial: the training set helps the model learn, while the validation set ensures that learning applies to new images.
</code></pre>
<ol>
<li><strong>Understanding the Loss Curves</strong>: During training, you’ll see two curves— training loss and validation loss. These curves represent how well the model is performing:</li>
<li><strong>Training Loss</strong>: Shows how well the model is learning on the images it’s trained on.</li>
<li><strong>Validation Loss</strong>: Indicates how well the model generalizes to new, unseen images.
    Ideally, both curves will <strong>decrease over time</strong>. If validation loss stops decreasing or begins to rise, it can mean the model is overfitting (learning too specifically to the training data), which may require more varied data or adjustments.</li>
<li><strong>Finish Training</strong>: Once the model completes training, it will be ready to use.<blockquote>
<p>Tip: If you have a larger set of annotated images, the model can learn more accurately, but training might take longer. Start with a small set, and as you grow comfortable, you can add more images and retrain.</p>
</blockquote>
</li>
</ol>
<h3 id="step-3-inference-using-the-model-to-detect-objects-in-new-images">Step 3: Inference – Using the Model to Detect Objects in New Images</h3>
<p>Inference is when the trained model applies what it’s learned to new, unlabeled images. Here, the model will identify and label objects on its own based on the patterns it learned during training.
1. <strong>Select New Images</strong>: Place new images where you want the model to detect objects automatically into <code>ez-frcnn/test_data/test_images</code>.
2. <strong>Run Inference</strong>: Select the <code>Run Inference</code> option in EZ-FRCNN. The model will process your images and label objects based on your training.
3. <strong>Review the Results</strong>: After inference completes, you’ll see boxes around the detected objects in your images, along with confidence scores.
   - <strong>Confidence Score</strong>: This number (from 0 to 1) shows the model’s certainty about each detection, where higher scores mean greater confidence in the label.
5. <strong>CSV Output</strong>: The tool also generates a CSV file listing each image name, detected object, and the confidence score. This provides a quick overview and easy access to results for further analysis.</p>
<blockquote>
<p>Tip: If the results aren’t as accurate as you’d like, consider adding more annotations and retraining the model.</p>
</blockquote>
<h3 id="additional-tips-for-success">Additional Tips for Success</h3>
<ol>
<li><strong>Start Small</strong>: Begin with a small number of images and labels. As you gain experience, you can add more data to improve accuracy.</li>
<li><strong>Use Clear Images</strong>: The clearer and higher quality your images, the better your model will perform.</li>
<li><strong>Iterate</strong>: Machine learning models improve with iteration. Each round of annotation, training, and inference makes the model a little better!</li>
</ol>
<p>EZ-FRCNN was built to make machine learning accessible and user-friendly. Follow these steps, experiment, and soon you’ll have a trained model that recognizes your objects of interest with minimal effort!</p>
<h2 id="references">References</h2>
<p>EZ-FRCNN is an implementation of Faster R-CNN, an algorithm developed by <a href="https://ieeexplore.ieee.org/document/7485869">Ren et al</a>.</p>
<p>Written by Jacob Wheelock and Erin Shappell for Lu Lab, 2025.</p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="annotation/" class="btn btn-neutral float-right" title="ez-frcnn.annotation">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
    
      <span><a href="annotation/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="js/jquery-3.6.0.min.js"></script>
    <script>var base_url = ".";</script>
    <script src="js/theme_extra.js"></script>
    <script src="js/theme.js"></script>
      <script src="search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>

<!--
MkDocs version : 1.6.1
Build Date UTC : 2025-06-03 19:16:10.204691+00:00
-->
